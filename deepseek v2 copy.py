import os
import asyncio
import logging
from dotenv import load_dotenv
from aiogram import Bot, Dispatcher, types
from aiogram.types import Message
from aiogram.filters import Command
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import StatesGroup, State
from PyPDF2 import PdfReader
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI  
from aiogram import Router

# ===–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è===
logging.basicConfig(level=logging.DEBUG)

# ===–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è===
load_dotenv()
TOKEN = os.getenv("TELEGRAM_TOKEN")
PDF_FOLDER = os.getenv("PDF_FOLDER")
OPENAI_API_KEY = "sk-aitunnel-v66J7WhiyFEFEu2fQ32zLxgdh77VudJn"  
OPENAI_API_BASE = "https://api.aitunnel.ru/v1/"  

if not TOKEN:
    raise ValueError("–û—à–∏–±–∫–∞: TELEGRAM_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env!")
if not PDF_FOLDER:
    raise ValueError("–û—à–∏–±–∫–∞: PDF_FOLDER –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env!")

# ===–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞===
bot = Bot(token=TOKEN)
dp = Dispatcher()

# ===–°–æ—Å—Ç–æ—è–Ω–∏—è –±–æ—Ç–∞===
class SearchState(StatesGroup):
    searching = State()

#=== –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF===
async def extract_text_from_pdf(pdf_path):
    try:
        pdf_reader = PdfReader(pdf_path)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
        return text
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {pdf_path}: {e}")
        return ""

#=== –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö===
async def create_embeddings():
    logging.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏...")

    property_data = []
    metadata = []

    for filename in os.listdir(PDF_FOLDER):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(PDF_FOLDER, filename)
            text = await extract_text_from_pdf(pdf_path)

            property_data.append(text)
            metadata.append({"filename": filename})

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        length_function=len
    )

    chunks = []
    meta_list = []
    for text, meta in zip(property_data, metadata):
        prop_chunks = text_splitter.split_text(text)
        chunks.extend(prop_chunks)
        meta_list.extend([meta] * len(prop_chunks))

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )

    knowledge_base = FAISS.from_texts(chunks, embeddings, metadatas=meta_list)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    knowledge_base.save_local("faiss_index")
    logging.info("‚úÖ –ë–∞–∑–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    return knowledge_base

# ===–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö===
def load_embeddings():
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )
    
    if os.path.exists("faiss_index"):
        knowledge_base = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True  
        )
        logging.info("‚úÖ –ë–∞–∑–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞!")
    else:
        knowledge_base = None
        logging.warning("‚ö†Ô∏è –ë–∞–∑–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –µ—ë.")
    
    return knowledge_base

# === —Ä–æ—É—Ç–µ—Ä===
router = Router()

# ===–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ LangChain===
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ===–ü—Ä–æ–º–ø—Ç –¥–ª—è RAG===
template = """Eres un asistente de bienes ra√≠ces. Tu tarea es vender casas de los documentos. Usa los siguientes documentos para responder a la pregunta. Si no hay opciones adecuadas, dilo.

–î–æ–∫—É–º–µ–Ω—Ç—ã:
{context}

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:"""
QA_PROMPT = PromptTemplate.from_template(template)

#=== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChatOpenAI —Å –ø—Ä–æ–∫—Å–∏===
llm = ChatOpenAI(
    model="gpt-4o-mini",  
    temperature=0,  
    openai_api_key=OPENAI_API_KEY,  
    openai_api_base=OPENAI_API_BASE  
)

# ===–ö–æ–º–∞–Ω–¥–∞ /start===
@router.message(Command("start"))
async def start_cmd(message: Message, state: FSMContext):
    await state.clear()  
    await message.answer("Hola! Soy Angela, tu asistente de bienes ra√≠ces. üè°\n Qu√© tipo de propiedad est√°s buscando?")
    await state.set_state(SearchState.searching)

#===–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è===
@router.message(SearchState.searching)
async def handle_search(message: Message, state: FSMContext):
    try:
        user_query = message.text

        # === RAG –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        retriever = knowledge_base.as_retriever(search_kwargs={"k": knowledge_base.index.ntotal})
        docs = retriever.get_relevant_documents(user_query)  # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        # ===–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context = "\n\n".join([doc.page_content for doc in docs])

        # === –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –∏–∑ –ø–∞–º—è—Ç–∏
        chat_history = memory.load_memory_variables({})["chat_history"]

        # ===–ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–µ–π –∏ –≤–æ–ø—Ä–æ—Å–æ–º
        full_prompt = f"Historia del di√°logo:\n{chat_history}\n\nDocumentos:\n{context}\n\nPregunta: {user_query}\nRespuesta:"

        # ===ChatOpenAI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        response = llm.invoke(full_prompt)

        # === —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏–∏
        memory.save_context({"question": user_query}, {"answer": response.content})

        # ===–æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        await message.answer(response.content, parse_mode="Markdown")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        await message.answer("Ocurri√≥ un error al procesar su solicitud. Por favor, int√©ntelo m√°s tarde.")

# ===–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞===
async def main():
    global knowledge_base
    logging.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏...")
    knowledge_base = load_embeddings()  # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    if knowledge_base is None:
        knowledge_base = await create_embeddings()  # –ï—Å–ª–∏ –±–∞–∑—ã –Ω–µ—Ç, —Å–æ–∑–¥–∞—ë–º –µ—ë

    logging.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Telegram-–±–æ—Ç–∞...")
    dp.include_router(router)
    await dp.start_polling(bot)

if __name__ == "__main__":
    logging.info("üü¢ –ó–∞–ø—É—Å–∫–∞–µ–º event loop...")
    asyncio.run(main())