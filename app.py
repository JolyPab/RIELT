import os
import asyncio
import logging
from dotenv import load_dotenv
from aiogram import Bot, Dispatcher, types
from aiogram.types import Message, FSInputFile
from aiogram.filters import Command
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import StatesGroup, State
from PyPDF2 import PdfReader
import requests
import torch  
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from openai import OpenAI  # OpenAI Proxy API 
from aiogram import Router

logging.basicConfig(level=logging.DEBUG)

#=== –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è ===
load_dotenv()
TOKEN = os.getenv("TELEGRAM_TOKEN")
PDF_FOLDER = os.getenv("PDF_FOLDER")

if not TOKEN:
    raise ValueError("–û—à–∏–±–∫–∞: TELEGRAM_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env!")
if not PDF_FOLDER:
    raise ValueError("–û—à–∏–±–∫–∞: PDF_FOLDER –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env!")

bot = Bot(token=TOKEN)
dp = Dispatcher()

#=== OpenAI —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ AITunnel ===
client = OpenAI(
    api_key="sk-aitunnel-v66J7WhiyFEFEu2fQ32zLxgdh77VudJn",
    base_url="https://api.aitunnel.ru/v1/"
)

class SearchState(StatesGroup):
    searching = State()
    last_result = State()  # –ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞




#=================================
#=== üõ† –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ GPT ===
#=================================
async def extract_criteria(user_query):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (–±—é–¥–∂–µ—Ç, –∫–æ–º–Ω–∞—Ç—ã, –±–∞—Å—Å–µ–π–Ω –∏ —Ç. –¥.)"""
    messages = [
        {"role": "system", "content": "–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏. –í—ã–¥–µ–ª–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–∏ (–±—é–¥–∂–µ—Ç, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç, –±–∞—Å—Å–µ–π–Ω, —Ä–∞–π–æ–Ω –∏ —Ç. –¥.). –û—Ç–≤–µ—Ç—å JSON-–æ–±—ä–µ–∫—Ç–æ–º."},
        {"role": "user", "content": f"–ö–ª–∏–µ–Ω—Ç –Ω–∞–ø–∏—Å–∞–ª: {user_query}"}
    ]

    response = client.chat.completions.create(
        messages=messages,
        max_tokens=100,
        model="gpt-3.5-turbo"
    )

    criteria_text = response.choices[0].message.content
    try:
        criteria = eval(criteria_text)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º JSON-–æ—Ç–≤–µ—Ç –≤ Python-–æ–±—ä–µ–∫—Ç
    except Exception:
        criteria = {}

    return criteria

#=================================
#=== üîç –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ ===
#=================================
@dp.message(SearchState.searching)
async def search_properties(message: Message, state: FSMContext):
    print(f"üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {message.text}")  # –û—Ç–ª–∞–¥–∫–∞
    await message.answer("–Ø –ø–æ–ª—É—á–∏–ª —Ç–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ!")
    user_query = message.text

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±—é–¥–∂–µ—Ç, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç)
    criteria = await extract_criteria(user_query)

    # –ü–æ–∏—Å–∫ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤ –±–∞–∑–µ
    docs = knowledge_base.similarity_search(user_query, 3) if knowledge_base else []

    if docs:
        first_property = docs[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç

        properties = "\n".join([
            f"üè† *{doc.metadata.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}*\nüìã {doc.page_content[:250]}..."
            for doc in docs
        ])

        # **–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç –≤ FSM**, —á—Ç–æ–±—ã –±–æ—Ç –µ–≥–æ –ø–æ–º–Ω–∏–ª
        await state.update_data(last_property=first_property)

        # **–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò**
        response = await generate_ai_response(user_query, properties)
        await message.answer(response)

    else:
        await message.answer("üòï –ù–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å.")



    # === –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç ===
    filtered_properties = []
    if not filtered_properties:
        response = "üòï –ù–µ –Ω–∞—à–µ–ª –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏."
    else:
        properties = "\n\n".join(filtered_properties)
        response = await generate_ai_response(user_query, properties)

    # === –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç ===
    await message.answer(response, parse_mode="Markdown")
    await message.answer("üîé –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –∏—Ö.")

#=================================
#=== ‚ú® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ GPT ===
#=================================
async def generate_ai_response(user_query, properties):
    messages = [
        {"role": "system", "content": "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –∂–∏–≤–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ. –î–æ–±–∞–≤–ª—è–π —ç–º–æ–¥–∑–∏, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º."},
        {"role": "user", "content": f"–ö–ª–∏–µ–Ω—Ç –∏—â–µ—Ç –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å. –ó–∞–ø—Ä–æ—Å: {user_query}"},
        {"role": "assistant", "content": f"–í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤:\n\n{properties}"},
        {"role": "user", "content": "–ù–∞–ø–∏—à–∏ –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ –∂–∏–≤–æ–≥–æ –æ–±—â–µ–Ω–∏—è, —Å —ç–º–æ—Ü–∏—è–º–∏ –∏ –ø–æ–ª–µ–∑–Ω—ã–º–∏ —Å–æ–≤–µ—Ç–∞–º–∏."}
    ]

    response = client.chat.completions.create(
        messages=messages,
        max_tokens=500,  
        model="gpt-3.5-turbo"
    )
    
    return response.choices[0].message.content



#=================================
#=== üîç –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ ===
#=================================
async def create_embeddings():
    print("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏...")

    property_data = []
    metadata = []

    # === –ß–∏—Ç–∞–µ–º –≤—Å–µ PDF –∏–∑ –ø–∞–ø–∫–∏ ===
    for filename in os.listdir(PDF_FOLDER):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(PDF_FOLDER, filename)
            pdf_reader = PdfReader(pdf_path)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()

            property_data.append(text)
            metadata.append({"filename": filename})

    # === –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è embeddings ===
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,   
        chunk_overlap=100,
        length_function=len
    )

    chunks = []
    meta_list = []
    for text, meta in zip(property_data, metadata):
        prop_chunks = text_splitter.split_text(text)
        chunks.extend(prop_chunks)
        meta_list.extend([meta] * len(prop_chunks))

    # === –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Hugging Face ===
    print("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Hugging Face...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )

    knowledge_base = FAISS.from_texts(chunks, embeddings, metadatas=meta_list)
    print("‚úÖ –ë–∞–∑–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    return knowledge_base


# === –°–æ–∑–¥–∞—ë–º —Ä–æ—É—Ç–µ—Ä ===
router = Router()

#=================================
#=== üè† –ö–æ–º–∞–Ω–¥–∞ /start ===
#=================================
@router.message(Command("start"))
async def start_cmd(message: Message, state: FSMContext):
    await message.answer("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ üè°\n–ö–∞–∫—É—é –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –≤—ã –∏—â–µ—Ç–µ?")
    await state.set_state(SearchState.searching)

@dp.message()
async def ask_details(message: Message, state: FSMContext):
    user_query = message.text.lower()
    
    if "–ø–æ–¥—Ä–æ–±–Ω–µ–µ" in user_query or "—Ä–∞—Å—Å–∫–∞–∂–∏" in user_query:
        data = await state.get_data()
        last_property = data.get("last_property")

        if last_property:
            await message.answer(f"üìå –í–æ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –ø–æ {last_property.metadata.get('filename', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–º—É –æ–±—ä–µ–∫—Ç—É')}:\n\n"
                                 f"üìã {last_property.page_content[:500]}...\n\n"
                                 "–•–æ—á–µ—à—å —É–∑–Ω–∞—Ç—å —á—Ç–æ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ? üòâ")
        else:
            await message.answer("üòï –ù–µ –º–æ–≥—É –≤—Å–ø–æ–º–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –∑–∞–Ω–æ–≤–æ.")


#=================================
#=== üöÄ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ===
#=================================
async def main():
    global knowledge_base
    print("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏...")
    knowledge_base = await create_embeddings()
    print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Telegram-–±–æ—Ç–∞...")

    dp.include_router(router)  # üí° –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ö–µ–Ω–¥–ª–µ—Ä—ã
    await dp.start_polling(bot)

if __name__ == "__main__":
    print("üü¢ –ó–∞–ø—É—Å–∫–∞–µ–º event loop...")
    asyncio.run(main()) curl https://api.telegram.org/bot<8041779345:AAE41dOCSlRCCjL63L-QaS9IghHE9XvQkp0>/getMe
